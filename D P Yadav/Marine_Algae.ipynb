{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, concatenate, Input, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import layers,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('acc') >= 0.96):   \n",
    "            print(\"Reached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS =100\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "default_image_size = tuple((224, 224))\n",
    "#image_size = 0\n",
    "width=224\n",
    "height=2224\n",
    "depth=3\n",
    "train_dir=r\"C:\\Users\\Glau\\Desktop\\marine_new\\Augmentation\\Train\"\n",
    "valid_dir=r\"C:\\Users\\Glau\\Desktop\\marine_new\\Augmentation\\Test\"\n",
    "train_folder=listdir(train_dir)\n",
    "valid_folder=listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = MyCustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Amphidinium carterae ...\n",
      "processing Chatonella marina ...\n",
      "processing Cochlodinium ...\n",
      "processing Coolia monotis ...\n",
      "processing Dinophysis forti ...\n",
      "processing Gambierdiscus toxicus ...\n",
      "processing Gymnodinium catenatum ...\n",
      "processing Karenia brevis ...\n",
      "processing Karlodinium veneficum ...\n",
      "processing Lyngbya majuscula ...\n",
      "processing Ostreopsis siamensis ...\n",
      "processing Protoceratium reticulatum ...\n",
      "processing Prymnesium parvum ...\n",
      "processing Pseudo-nitzschia fraudulenta ...\n",
      "processing Tolypothrix conglutinata ...\n",
      "[INFO] Image loading completed\n"
     ]
    }
   ],
   "source": [
    "train_image_list, train_image_label= [], []\n",
    "for disease_folder in train_folder:\n",
    "    print(f\"processing {disease_folder} ...\")\n",
    "    disease_img_folder= listdir(f\"{train_dir}/{disease_folder}\")\n",
    "    #print(disease_img_folder)\n",
    "    for disease_img in disease_img_folder:\n",
    "    #for disease_img in disease_img_folder[: : 2]:    \n",
    "        image_directory = f\"{train_dir}/{disease_folder}/{disease_img}\"\n",
    "        if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".TIFF\") == True:\n",
    "            train_image_list.append(convert_image_to_array(image_directory))\n",
    "            train_image_label.append(disease_folder)\n",
    "print(\"[INFO] Image loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Amphidinium carterae ...\n",
      "processing Chatonella marina ...\n",
      "processing Cochlodinium polykrikoides ...\n",
      "processing Coolia monotis ...\n",
      "processing Dinophysis forti ...\n",
      "processing Gambierdiscus toxicus ...\n",
      "processing Gymnodinium catenatum ...\n",
      "processing Karenia brevis ...\n",
      "processing Karlodinium ...\n",
      "processing Lyngbya majuscula ...\n",
      "processing Ostreopsis siamensis ...\n",
      "processing Protoceratium reticulatum ...\n",
      "processing Prymnesium parvum ...\n",
      "processing Pseudo-nitzschia fradulenta ...\n",
      "processing Tolypothrix conglutinata ...\n",
      "[INFO] Image loading completed\n"
     ]
    }
   ],
   "source": [
    "valid_image_list, valid_image_label= [], []\n",
    "for disease_folder in valid_folder:\n",
    "    print(f\"processing {disease_folder} ...\")\n",
    "    disease_img_folder= listdir(f\"{valid_dir}/{disease_folder}\")\n",
    "    for disease_img in disease_img_folder:\n",
    "    #for disease_img in disease_img_folder[: : 2]: \n",
    "        image_directory = f\"{valid_dir}/{disease_folder}/{disease_img}\"\n",
    "        if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".TIFF\") == True:\n",
    "            valid_image_list.append(convert_image_to_array(image_directory))\n",
    "            valid_image_label.append(disease_folder)\n",
    "print(\"[INFO] Image loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "bin_train_image_labels = label_binarizer.fit_transform(train_image_label)\n",
    "bin_valid_image_labels = label_binarizer.fit_transform(valid_image_label)\n",
    "pickle.dump(label_binarizer,open('Label_Instance_marine20.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_image_list = np.array(train_image_list, dtype=np.float32) / 255.0\n",
    "np_valid_image_list = np.array(valid_image_list, dtype=np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "model = Sequential() \n",
    "  \n",
    "# 1st Convolutional Layer \n",
    "model.add(Conv2D(filters = 96, input_shape = (224, 224, 3),  \n",
    "            kernel_size = (11, 11), strides = (4, 4),  \n",
    "            padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Max-Pooling  \n",
    "model.add(MaxPooling2D(pool_size = (2, 2), \n",
    "            strides = (2, 2), padding = 'valid')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 2nd Convolutional Layer \n",
    "model.add(Conv2D(filters = 256, kernel_size = (11, 11),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Max-Pooling \n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2),  \n",
    "            padding = 'valid')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 3rd Convolutional Layer \n",
    "model.add(Conv2D(filters = 384, kernel_size = (3, 3),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 4th Convolutional Layer \n",
    "model.add(Conv2D(filters = 384, kernel_size = (3, 3),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 5th Convolutional Layer \n",
    "model.add(Conv2D(filters = 256, kernel_size = (3, 3),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Max-Pooling \n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2),  \n",
    "            padding = 'valid')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# Flattening \n",
    "model.add(Flatten()) \n",
    "  \n",
    "# 1st Dense Layer \n",
    "model.add(Dense(4096, input_shape = (224*224*3, ))) \n",
    "model.add(Activation('relu')) \n",
    "# Add Dropout to prevent overfitting \n",
    "model.add(Dropout(0.4)) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 2nd Dense Layer \n",
    "#model.add(Dense(4096)) \n",
    "#model.add(Activation('relu')) \n",
    "# Add Dropout \n",
    "#model.add(Dropout(0.4)) \n",
    "# Batch Normalisation \n",
    "#model.add(BatchNormalization()) \n",
    "  \n",
    "# Output Softmax Layer \n",
    "model.add(Dense(15)) \n",
    "model.add(Activation('softmax')) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "#Instantiate an empty model\n",
    "model = Sequential([\n",
    "Conv2D(64, (3, 3), input_shape=input_shape, padding='same, activation='relu'),\n",
    "Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(128, (3, 3), activation=’relu’, padding=’same’),\n",
    "Conv2D(128, (3, 3), activation=’relu’, padding=’same’,),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(256, (3, 3), activation=’relu’, padding=’same’,),\n",
    "Conv2D(256, (3, 3), activation=’relu’, padding=’same’,),\n",
    "Conv2D(256, (3, 3), activation=’relu’, padding=’same’,),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation=’relu’, padding=’same’,),\n",
    "Conv2D(512, (3, 3), activation=’relu’, padding=’same’,),\n",
    "Conv2D(512, (3, 3), activation=’relu’, padding=’same’,),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation=’relu’, padding=’same’,),\n",
    "Conv2D(512, (3, 3), activation=’relu’, padding=’same’,),\n",
    "Conv2D(512, (3, 3), activation=’relu’, padding=’same’,),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Flatten(),\n",
    "Dense(4096, activation=’relu’),\n",
    "Dense(4096, activation=’relu’),\n",
    "Dense(1000, activation=’softmax’)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=’adam’, metrics=[“accuracy”])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                61455     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15)                0         \n",
      "=================================================================\n",
      "Total params: 24,040,207\n",
      "Trainable params: 24,021,071\n",
      "Non-trainable params: 19,136\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "#Instantiate an empty model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding=’valid’))\n",
    "model.add(Activation(‘relu’))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding=’valid’))\n",
    "model.add(Activation(‘relu’))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
    "model.add(Activation(‘relu’))\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
    "model.add(Activation(‘relu’))\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
    "model.add(Activation(‘relu’))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "model.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation(‘relu’))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 2nd Fully Connected Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation(‘relu’))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 3rd Fully Connected Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation(‘relu’))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation(‘softmax’))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=’adam’, metrics=[“accuracy”])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"[INFO] training network...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "72000/72000 [==============================] - 143s 2ms/step - loss: 0.7122 - accuracy: 0.8020 - val_loss: 0.2445 - val_accuracy: 0.9463\n",
      "Epoch 2/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.1854 - accuracy: 0.9541 - val_loss: 0.3072 - val_accuracy: 0.9538\n",
      "Epoch 3/100\n",
      "72000/72000 [==============================] - 149s 2ms/step - loss: 0.1355 - accuracy: 0.9710 - val_loss: 0.0192 - val_accuracy: 0.9956\n",
      "Epoch 4/100\n",
      "72000/72000 [==============================] - 146s 2ms/step - loss: 0.0999 - accuracy: 0.9800 - val_loss: 0.0050 - val_accuracy: 0.9980\n",
      "Epoch 5/100\n",
      "72000/72000 [==============================] - 146s 2ms/step - loss: 0.0742 - accuracy: 0.9860 - val_loss: 0.0094 - val_accuracy: 0.9977\n",
      "Epoch 6/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0545 - accuracy: 0.9896 - val_loss: 0.0029 - val_accuracy: 0.9995\n",
      "Epoch 7/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0563 - accuracy: 0.9902 - val_loss: 0.1543 - val_accuracy: 0.9738\n",
      "Epoch 8/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0467 - accuracy: 0.9918 - val_loss: 0.0011 - val_accuracy: 0.9994\n",
      "Epoch 9/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0430 - accuracy: 0.9929 - val_loss: 0.0012 - val_accuracy: 0.9996\n",
      "Epoch 10/100\n",
      "72000/72000 [==============================] - 146s 2ms/step - loss: 0.0300 - accuracy: 0.9952 - val_loss: 0.0182 - val_accuracy: 0.9968\n",
      "Epoch 11/100\n",
      "72000/72000 [==============================] - 145s 2ms/step - loss: 0.0270 - accuracy: 0.9953 - val_loss: 0.0049 - val_accuracy: 0.9988\n",
      "Epoch 12/100\n",
      "72000/72000 [==============================] - 146s 2ms/step - loss: 0.0313 - accuracy: 0.9944 - val_loss: 0.0109 - val_accuracy: 0.9976\n",
      "Epoch 13/100\n",
      "72000/72000 [==============================] - 148s 2ms/step - loss: 0.0225 - accuracy: 0.9963 - val_loss: 9.5657e-05 - val_accuracy: 0.9999\n",
      "Epoch 14/100\n",
      "72000/72000 [==============================] - 148s 2ms/step - loss: 0.0281 - accuracy: 0.9959 - val_loss: 0.2685 - val_accuracy: 0.9721\n",
      "Epoch 15/100\n",
      "72000/72000 [==============================] - 148s 2ms/step - loss: 0.0171 - accuracy: 0.9968 - val_loss: 5.3055e-04 - val_accuracy: 0.9997\n",
      "Epoch 16/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0165 - accuracy: 0.9970 - val_loss: 0.0042 - val_accuracy: 0.9990\n",
      "Epoch 17/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0182 - accuracy: 0.9971 - val_loss: 8.2876e-04 - val_accuracy: 0.9998\n",
      "Epoch 18/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0172 - accuracy: 0.9973 - val_loss: 0.3627 - val_accuracy: 0.9743\n",
      "Epoch 19/100\n",
      "72000/72000 [==============================] - 146s 2ms/step - loss: 0.0152 - accuracy: 0.9977 - val_loss: 3.2290e-08 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0132 - accuracy: 0.9980 - val_loss: 0.0064 - val_accuracy: 0.9991\n",
      "Epoch 21/100\n",
      "72000/72000 [==============================] - 147s 2ms/step - loss: 0.0127 - accuracy: 0.9977 - val_loss: 8.0671e-05 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "72000/72000 [==============================] - 148s 2ms/step - loss: 0.0120 - accuracy: 0.9981 - val_loss: 8.7562e-06 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "71968/72000 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9980"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-74df9095693b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_valid_image_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_valid_image_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                   )\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow20\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow20\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                                          verbose=0)\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow20\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow20\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow20\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit(np_train_image_list,bin_train_image_labels,\n",
    "                  validation_data=(np_valid_image_list, bin_valid_image_labels),\n",
    "                  batch_size=BS,\n",
    "                  epochs=EPOCHS, verbose=1        \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
