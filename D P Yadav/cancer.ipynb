{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "#from vis.utils import utils\n",
    "import numpy as np\n",
    "from scipy import misc,ndimage\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " batch_size=1\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                        \n",
    "                                         horizontal_flip = True,\n",
    "                                         vertical_flip = True,\n",
    "                                         width_shift_range = .2,\n",
    "                                         height_shift_range =.2,\n",
    "                                         zoom_range = .1 )\n",
    "        \n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'C:\\\\Users\\\\Glau\\\\Desktop\\\\DP\\\\\\Single_fifteen\\\\EBO1\\\\',  # this is the target directory\n",
    "        target_size=(400, 400),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,shuffle=True)  # since we use binary_crossentropy loss, we need binary label\n",
    "print(len(train_generator))\n",
    "aug_image=[next(train_generator)[0].astype(np.uint8) for i in range(100)]\n",
    "plt.imshow(aug_image[0])\n",
    "num=8901\n",
    "path='C:\\\\Users\\\\Glau\\\\Desktop\\\\DP\\\\\\Single_fifteen\\\\'\n",
    "\n",
    "for i in aug_image:\n",
    "    cv2.imwrite(os.path.join(path)+'EBO_'+str(num)+'.tiff',i)\n",
    "    num=num+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate pythonGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir ='C:\\\\Users\\\\Glau\\\\Desktop\\\\DP\\\\MON\\\\'\n",
    "c=0\n",
    "for i in os.listdir(datadir):\n",
    "    im=cv2.imread(os.path.join(datadir,i))\n",
    "    c=c+1\n",
    "print(c)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "datadir ='C:\\\\Users\\\\Glau\\\\Desktop\\\\DP\\\\single_fifteen\\\\Train'\n",
    "\n",
    "print(os.listdir(datadir))\n",
    "\n",
    "cat=['BAS', 'EBO', 'EOS', 'KSC', 'LYA', 'LYT', 'MMZ', 'MOB', 'MON', 'MYB', 'MYO', 'NGB', 'NGS', 'PMB', 'PMO']\n",
    "for category in cat:\n",
    "    path=os.path.join(datadir,category)\n",
    "    print(path)\n",
    "    for img in os.listdir(path):\n",
    "        #print(path)\n",
    "        img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "        #img_array = filters.sobel(img_array)\n",
    "        #img_array.max() \n",
    "        #plt.imshow(img_array,cmap='gray')\n",
    "         \n",
    "        #plt.show()\n",
    "        #threshold = skimage.filters.threshold_otsu(img_array)\n",
    "        #print('Threshold value is {}'.format(threshold))\n",
    "       # predicted = np.uint8(img_array > threshold) * 255\n",
    "       # plt.imshow(predicted, cmap='gray')\n",
    "        #plt.axis('off')\n",
    "        #plt.title('otsu predicted binary image')\n",
    "        #img_array=predicted\n",
    "        #image_felzenszwalb = seg.felzenszwalb(img_array) \n",
    "        #np.unique(image_felzenszwalb).size\n",
    "        #image_felzenszwalb_colored = color.label2rgb(image_felzenszwalb, img_array, kind='avg')\n",
    "        #plt.imshow(image_felzenszwalb_colored);\n",
    "        #img_array=image_felzenszwalb_colored\n",
    "        \n",
    "        \n",
    "        break\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=400\n",
    "training_data=[]\n",
    "#catego=['healthy','cancer']\n",
    "def create_training_data():\n",
    "    for category in cat:\n",
    "        path=os.path.join(datadir,category)\n",
    "        class_num=cat.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                #new_array=cv2.resize(img_array,(img_size,img_size))\n",
    "                \n",
    "                training_data.append([img_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=[]\n",
    "Y1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training,label in training_data:\n",
    "    X1.append(training)\n",
    "    Y1.append(label)\n",
    "X1=np.array(X1)\n",
    "Y1=np.array(Y1)\n",
    "print(len(X1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "datadir1 ='C:\\\\Users\\\\Glau\\\\Desktop\\\\DP\\\\single_fifteen\\\\Test\\\\'\n",
    "\n",
    "print(os.listdir(datadir1))\n",
    "\n",
    "cat1=['BAS', 'EBO', 'EOS', 'KSC', 'LYA', 'LYT', 'MMZ', 'MOB', 'MON', 'MYB', 'MYO', 'NGB', 'NGS', 'PMB', 'PMO']\n",
    "for category in cat1:\n",
    "    path1=os.path.join(datadir1,category)\n",
    "    print(path1)\n",
    "    for img in os.listdir(path1):\n",
    "        #print(path)\n",
    "        img_array1=cv2.imread(os.path.join(path1,img),cv2.IMREAD_GRAYSCALE)\n",
    "        #img_array = filters.sobel(img_array)\n",
    "        #img_array.max() \n",
    "        #plt.imshow(img_array,cmap='gray')\n",
    "         \n",
    "        #plt.show()\n",
    "        #threshold = skimage.filters.threshold_otsu(img_array)\n",
    "        #print('Threshold value is {}'.format(threshold))\n",
    "       # predicted = np.uint8(img_array > threshold) * 255\n",
    "       # plt.imshow(predicted, cmap='gray')\n",
    "        #plt.axis('off')\n",
    "        #plt.title('otsu predicted binary image')\n",
    "        #img_array=predicted\n",
    "        #image_felzenszwalb = seg.felzenszwalb(img_array) \n",
    "        #np.unique(image_felzenszwalb).size\n",
    "        #image_felzenszwalb_colored = color.label2rgb(image_felzenszwalb, img_array, kind='avg')\n",
    "        #plt.imshow(image_felzenszwalb_colored);\n",
    "        #img_array=image_felzenszwalb_colored\n",
    "        \n",
    "        \n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=400\n",
    "test_data=[]\n",
    "#catego=['healthy','cancer']\n",
    "def create_test_data():\n",
    "    for category in cat1:\n",
    "        path1=os.path.join(datadir1,category)\n",
    "        class_num1=cat1.index(category)\n",
    "        for img in os.listdir(path1):\n",
    "            try:\n",
    "                img_array1=cv2.imread(os.path.join(path1,img),cv2.IMREAD_GRAYSCALE)\n",
    "                #new_array=cv2.resize(img_array,(img_size,img_size))\n",
    "                \n",
    "                test_data.append([img_array1,class_num1])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "\n",
    "create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=[]\n",
    "Y2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test,label in test_data:\n",
    "    X2.append(test)\n",
    "    Y2.append(label)\n",
    "X2=np.array(X2)\n",
    "Y2=np.array(Y2)\n",
    "print(len(X2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.layers import batch_norm, flatten\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "\n",
    "init_learning_rate = 0.1\n",
    "cardinality = 32 # how many split ?\n",
    "blocks = 3 # res_block ! (split + transition)\n",
    "depth = 64 # out channel\n",
    "\n",
    "\"\"\"\n",
    "So, the total number of layers is (3*blokcs)*residual_layer_num + 2\n",
    "because, blocks = split(conv 2) + transition(conv 1) = 3 layer\n",
    "and, first conv layer 1, last dense layer 1\n",
    "thus, total number of layers = (3*blocks)*residual_layer_num + 2\n",
    "\"\"\"\n",
    "\n",
    "reduction_ratio = 4\n",
    "\n",
    "batch_size = 128\n",
    "iteration = 391\n",
    "# 128 * 391 ~ 50,000\n",
    "\n",
    "test_iteration = 15\n",
    "\n",
    "total_epochs = 100\n",
    "\n",
    "def conv_layer(input, filter, kernel, stride, padding='SAME', layer_name=\"conv\"):\n",
    "    with tf.name_scope(layer_name):\n",
    "        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n",
    "        return network\n",
    "\n",
    "def Global_Average_Pooling(x):\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "\n",
    "def Average_pooling(x, pool_size=[2,2], stride=2, padding='SAME'):\n",
    "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Sigmoid(x) :\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def Concatenation(layers) :\n",
    "    return tf.concat(layers, axis=3)\n",
    "\n",
    "def Fully_connected(x, units=class_num, layer_name='fully_connected') :\n",
    "    with tf.name_scope(layer_name) :\n",
    "        return tf.layers.dense(inputs=x, use_bias=False, units=units)\n",
    "\n",
    "def Evaluate(sess):\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_pre_index = 0\n",
    "    add = 1000\n",
    "\n",
    "    for it in range(test_iteration):\n",
    "        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n",
    "        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n",
    "        test_pre_index = test_pre_index + add\n",
    "\n",
    "        test_feed_dict = {\n",
    "            x: test_batch_x,\n",
    "            label: test_batch_y,\n",
    "            learning_rate: epoch_learning_rate,\n",
    "            training_flag: False\n",
    "        }\n",
    "\n",
    "        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
    "\n",
    "        test_loss += loss_\n",
    "        test_acc += acc_\n",
    "\n",
    "    test_loss /= test_iteration # average loss\n",
    "    test_acc /= test_iteration # average accuracy\n",
    "\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_loss),\n",
    "                                tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
    "\n",
    "    return test_acc, test_loss, summary\n",
    "\n",
    "class SE_ResNeXt():\n",
    "    def __init__(self, x, training):\n",
    "        self.training = training\n",
    "        self.model = self.Build_SEnet(x)\n",
    "\n",
    "    def first_layer(self, x, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            x = conv_layer(x, filter=64, kernel=[3, 3], stride=1, layer_name=scope+'_conv1')\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def transform_layer(self, x, stride, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            x = conv_layer(x, filter=depth, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            x = conv_layer(x, filter=depth, kernel=[3,3], stride=stride, layer_name=scope+'_conv2')\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch2')\n",
    "            x = Relu(x)\n",
    "            return x\n",
    "\n",
    "    def transition_layer(self, x, out_dim, scope):\n",
    "        with tf.name_scope(scope):\n",
    "            x = conv_layer(x, filter=out_dim, kernel=[1,1], stride=1, layer_name=scope+'_conv1')\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            # x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def split_layer(self, input_x, stride, layer_name):\n",
    "        with tf.name_scope(layer_name) :\n",
    "            layers_split = list()\n",
    "            for i in range(cardinality) :\n",
    "                splits = self.transform_layer(input_x, stride=stride, scope=layer_name + '_splitN_' + str(i))\n",
    "                layers_split.append(splits)\n",
    "\n",
    "            return Concatenation(layers_split)\n",
    "\n",
    "    def squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n",
    "        with tf.name_scope(layer_name) :\n",
    "\n",
    "\n",
    "            squeeze = Global_Average_Pooling(input_x)\n",
    "\n",
    "            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
    "            excitation = Relu(excitation)\n",
    "            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
    "            excitation = Sigmoid(excitation)\n",
    "\n",
    "            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "            scale = input_x * excitation\n",
    "\n",
    "            return scale\n",
    "\n",
    "    def residual_layer(self, input_x, out_dim, layer_num, res_block=blocks):\n",
    "        # split + transform(bottleneck) + transition + merge\n",
    "        # input_dim = input_x.get_shape().as_list()[-1]\n",
    "\n",
    "        for i in range(res_block):\n",
    "            input_dim = int(np.shape(input_x)[-1])\n",
    "\n",
    "            if input_dim * 2 == out_dim:\n",
    "                flag = True\n",
    "                stride = 2\n",
    "                channel = input_dim // 2\n",
    "            else:\n",
    "                flag = False\n",
    "                stride = 1\n",
    "\n",
    "            x = self.split_layer(input_x, stride=stride, layer_name='split_layer_'+layer_num+'_'+str(i))\n",
    "            x = self.transition_layer(x, out_dim=out_dim, scope='trans_layer_'+layer_num+'_'+str(i))\n",
    "            x = self.squeeze_excitation_layer(x, out_dim=out_dim, ratio=reduction_ratio, layer_name='squeeze_layer_'+layer_num+'_'+str(i))\n",
    "\n",
    "            if flag is True :\n",
    "                pad_input_x = Average_pooling(input_x)\n",
    "                pad_input_x = tf.pad(pad_input_x, [[0, 0], [0, 0], [0, 0], [channel, channel]]) # [?, height, width, channel]\n",
    "            else :\n",
    "                pad_input_x = input_x\n",
    "\n",
    "            input_x = Relu(x + pad_input_x)\n",
    "\n",
    "        return input_x\n",
    "\n",
    "\n",
    "    def Build_SEnet(self, input_x):\n",
    "        # only cifar10 architecture\n",
    "\n",
    "        input_x = self.first_layer(input_x, scope='first_layer')\n",
    "\n",
    "        x = self.residual_layer(input_x, out_dim=64, layer_num='1')\n",
    "        x = self.residual_layer(x, out_dim=128, layer_num='2')\n",
    "        x = self.residual_layer(x, out_dim=256, layer_num='3')\n",
    "\n",
    "        x = Global_Average_Pooling(x)\n",
    "        x = flatten(x)\n",
    "\n",
    "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
    "        return x\n",
    "\n",
    "\n",
    "train_x, train_y, test_x, test_y = prepare_data()\n",
    "train_x, test_x = color_preprocessing(train_x, test_x)\n",
    "\n",
    "\n",
    "# image_size = 32, img_channels = 3, class_num = 10 in cifar10\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\n",
    "label = tf.placeholder(tf.float32, shape=[None, class_num])\n",
    "\n",
    "training_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "logits = SE_ResNeXt(x, training=training_flag).model\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n",
    "\n",
    "l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
    "train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state('./model')\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "    epoch_learning_rate = init_learning_rate\n",
    "    for epoch in range(1, total_epochs + 1):\n",
    "        if epoch % 30 == 0 :\n",
    "            epoch_learning_rate = epoch_learning_rate / 10\n",
    "\n",
    "        pre_index = 0\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for step in range(1, iteration + 1):\n",
    "            if pre_index + batch_size < 50000:\n",
    "                batch_x = train_x[pre_index: pre_index + batch_size]\n",
    "                batch_y = train_y[pre_index: pre_index + batch_size]\n",
    "            else:\n",
    "                batch_x = train_x[pre_index:]\n",
    "                batch_y = train_y[pre_index:]\n",
    "\n",
    "            batch_x = data_augmentation(batch_x)\n",
    "\n",
    "            train_feed_dict = {\n",
    "                x: batch_x,\n",
    "                label: batch_y,\n",
    "                learning_rate: epoch_learning_rate,\n",
    "                training_flag: True\n",
    "            }\n",
    "\n",
    "            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
    "            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
    "\n",
    "            train_loss += batch_loss\n",
    "            train_acc += batch_acc\n",
    "            pre_index += batch_size\n",
    "\n",
    "\n",
    "        train_loss /= iteration # average loss\n",
    "        train_acc /= iteration # average accuracy\n",
    "\n",
    "        train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss),\n",
    "                                          tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])\n",
    "\n",
    "        test_acc, test_loss, test_summary = Evaluate(sess)\n",
    "\n",
    "        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
    "        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n",
    "        summary_writer.flush()\n",
    "\n",
    "        line = \"epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n\" % (\n",
    "            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n",
    "        print(line)\n",
    "\n",
    "        with open('logs.txt', 'a') as f:\n",
    "            f.write(line)\n",
    "\n",
    "        saver.save(sess=sess, save_path='./model/ResNeXt.ckpt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
