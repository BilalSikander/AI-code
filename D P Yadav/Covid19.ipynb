{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imutils\n",
    "!pip install image-classifiers==1.0.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.applications import VGG16, DenseNet169\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout, GlobalAveragePooling2D, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,TensorBoard,TerminateOnNaN, LearningRateScheduler\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.layers import Lambda, Reshape, DepthwiseConv2D, ZeroPadding2D, Add, MaxPooling2D,Activation, Flatten, Conv2D, Dense, Input, Dropout, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "#from classification_models.tfkeras import Classifiers\n",
    "from datetime import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('acc') >= 0.96):   \n",
    "            print(\"Reached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS =50 \n",
    "INIT_LR = 3e-3\n",
    "BS =32\n",
    "default_image_size = tuple((256, 256))\n",
    "image_size = 0\n",
    "width=256\n",
    "height=256\n",
    "depth=3\n",
    "train_dir=r\"C:\\Users\\Glau\\Desktop\\Covid_Radio\"\n",
    "#valid_dir=r\"C:\\Users\\Glau\\Desktop\\DP\\Pediastrum_cnn\\Test\"\n",
    "train_folder=listdir(train_dir)\n",
    "#valid_folder=listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = MyCustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_list, train_image_label=[],[]\n",
    "for disease_folder in train_folder:\n",
    "    print(f\"processing {disease_folder} ...\")\n",
    "    disease_img_folder= listdir(f\"{train_dir}/{disease_folder}\")\n",
    "    #print(disease_img_folder)\n",
    "    for disease_img in disease_img_folder:\n",
    "    #for disease_img in disease_img_folder[: : 2]:    \n",
    "        image_directory = f\"{train_dir}/{disease_folder}/{disease_img}\"\n",
    "        if image_directory.endswith(\".jpeg\") == True or image_directory.endswith(\".jpg\") == True or  image_directory.endswith(\".png\") == True:\n",
    "            train_image_list.append(convert_image_to_array(image_directory))\n",
    "            train_image_label.append(disease_folder)\n",
    "print(\"[INFO] Image loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "bin_train_image_labels = label_binarizer.fit_transform(train_image_label)\n",
    "#bin_valid_image_labels = label_binarizer.fit_transform(valid_image_label)\n",
    "pickle.dump(label_binarizer,open('Label_Instance_Covid_label3.pk', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bin_train_image_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_train_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_image_list = np.array(train_image_list, dtype=np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_train_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "#\n",
    "# image dimensions\n",
    "#\n",
    "\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_channels = 3\n",
    "\n",
    "#\n",
    "# network params\n",
    "#\n",
    "\n",
    "cardinality = 1\n",
    "\n",
    "\n",
    "def residual_network(x):\n",
    "    \"\"\"\n",
    "    ResNeX.t by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.ReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization,\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # conv1\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "\n",
    "    # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    for i in range(3):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 64, 128, _project_shortcut=project_shortcut)\n",
    "\n",
    "    # conv3\n",
    "    for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 128, 256, _strides=strides)\n",
    "\n",
    "    # conv4\n",
    "    for i in range(6):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 256, 1024, _strides=strides)\n",
    "\n",
    "    # conv5\n",
    "    for i in range(3):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 1024, 2048, _strides=strides)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(3,activation='softmax',name='fc3')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "network_output = residual_network(image_tensor)\n",
    "  \n",
    "model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"[INFO] training network...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,classification_report \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_list_of_arrays = []\n",
    "#loss_per_fold = []\n",
    "seed = 13   # for reproducibility you can change it. \n",
    "np.random.seed(seed)\n",
    "roundlabel=[]\n",
    "test=[]\n",
    "idtest=[]\n",
    "x_validate=[]\n",
    "kfold = KFold(5, True, seed)\n",
    "for train_idx, val_idx in kfold.split(np_train_image_list, y=bin_train_image_labels):\n",
    "    #print('train: %s, val: %s' % (train_idx, val_idx))\n",
    "    x_train, x_val = np_train_image_list[train_idx], np_train_image_list[val_idx]\n",
    "    y_train, y_val= bin_train_image_labels[train_idx], bin_train_image_labels[val_idx]\n",
    "    history=model.fit(x_train, y_train,\n",
    "                  validation_data=(x_val, y_val),\n",
    "                  batch_size=BS,\n",
    "                  epochs=EPOCHS, verbose=1        \n",
    "                  )\n",
    "    ycalculated = model.predict(x_val)\n",
    "    yhat_classes =np.argmax(ycalculated,axis=1)\n",
    "    rounded_labels=np.argmax(y_val,axis=1)                           \n",
    "    conf_matrix = confusion_matrix(rounded_labels,yhat_classes)\n",
    "    conf_matrix_list_of_arrays .append(conf_matrix)\n",
    "    roundlabel.append(rounded_labels)\n",
    "    test.append(x_val)\n",
    "    idtest.append(val_idx)\n",
    "    \n",
    "print('Score per fold')\n",
    "for i in range(0, len(conf_matrix_list_of_arrays)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1}  - Accuracy: {conf_matrix_list_of_arrays[i]}%')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gender.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c=0\n",
    "#for i in range(len(rounded_labels)):\n",
    "    #if(rounded_labels[i]==0):\n",
    "       # c=c+1\n",
    "#print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(roundlabel[0])):\n",
    "   # print(roundlabel[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i in range(len(test[0])):\n",
    "    #print(test[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c=0\n",
    "#for i in range(len(test[0])):\n",
    "    #if (test[0][i] == 0):\n",
    "       # print(i,end=\"   \")\n",
    "        #print(test[0][i])\n",
    "        #c+=1\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(idtest[0])):\n",
    "    #print(idtest[0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=conf_matrix_list_of_arrays[0]\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize = 'xx-large')\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "for i in range(5):\n",
    "    plot_confusion_matrix(conf_matrix_list_of_arrays[i], \n",
    "                      normalize = False,\n",
    "                      target_names = ['No-Findings', 'Pneumonia', 'Covid19'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report \n",
    "target_names = ['No-Findings', 'Pneumonia','Covid19']\n",
    "print(classification_report(rounded_labels, yhat_classes[0], target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "#cm = confusion_matrix(testY, np.argmax(predY, axis = -1))\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "#precision=\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label_map={0 : 'No-Findings',1:'Pneumonia ', 2: 'covid', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "n_classes =3\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 2\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(rounded_labels)\n",
    "y_test= lb.transform(rounded_labels)\n",
    "snn_pred = lb.transform(yhat_classes)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], snn_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), snn_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes-97), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(3), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], class_to_label_map[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw,color='white')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic of Cell')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#Train and validation accuracy\n",
    "plt.plot(epochs, acc, 'g', label='Training accurarcy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#Train and validation loss\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_layer=model.get_layer('add_15')\n",
    "model=Model(inputs=model.input,outputs =activation_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dense=model.get_layer('Dense')\n",
    "w=final_dense.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "\n",
    "model = ResNet50(weights='model_covidtest4.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1a2aa1b50e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "def ResNet(input_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Definition of ResNet\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n",
    "    \"\"\"\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    \n",
    "    bn_axis = 3\n",
    "    \n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='conv1')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "    \n",
    "    # the commented out blocks are what's needed to build out the\n",
    "    # full ResNet50 (a ResNet with 50 layers), we won't be needing\n",
    "    # the complexity here\n",
    "    # x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    img_output = layers.Dense(n_classes, activation='softmax', name='fc' + str(n_classes))(x)\n",
    "\n",
    "    model = Model(inputs=img_input, outputs=img_output, name='resnet')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = ResNet(input_shape, n_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "\tdef __init__(self, model, classIdx, layerName=None):\n",
    "\t\t# store the model, the class index used to measure the class\n",
    "\t\t# activation map, and the layer to be used when visualizing\n",
    "\t\t# the class activation map\n",
    "\t\tself.model = model\n",
    "\t\tself.classIdx = classIdx\n",
    "\t\tself.layerName = layerName\n",
    "\n",
    "\t\t# if the layer name is None, attempt to automatically find\n",
    "\t\t# the target output layer\n",
    "\t\tif self.layerName is None:\n",
    "\t\t\tself.layerName = self.find_target_layer()\n",
    "\n",
    "\tdef find_target_layer(self):\n",
    "\t\t# attempt to find the final convolutional layer in the network\n",
    "\t\t# by looping over the layers of the network in reverse order\n",
    "\t\tfor layer in reversed(self.model.layers):\n",
    "\t\t\t# check to see if the layer has a 4D output\n",
    "\t\t\tif len(layer.output_shape) == 4:\n",
    "\t\t\t\treturn layer.name\n",
    "\n",
    "\t\t# otherwise, we could not find a 4D layer so the GradCAM\n",
    "\t\t# algorithm cannot be applied\n",
    "\t\traise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "\n",
    "\tdef compute_heatmap(self, image, eps=1e-8):\n",
    "\t\t# construct our gradient model by supplying (1) the inputs\n",
    "\t\t# to our pre-trained model, (2) the output of the (presumably)\n",
    "\t\t# final 4D layer in the network, and (3) the output of the\n",
    "\t\t# softmax activations from the model\n",
    "\t\tgradModel = Model(\n",
    "\t\t\tinputs=[self.model.inputs],\n",
    "\t\t\toutputs=[self.model.get_layer(self.layerName).output, \n",
    "                     \n",
    "\t\t\t\tself.model.output])\n",
    "\n",
    "\t\t# record operations for automatic differentiation\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\t# cast the image tensor to a float-32 data type, pass the\n",
    "\t\t\t# image through the gradient model, and grab the loss\n",
    "\t\t\t# associated with the specific class index\n",
    "\t\t\tinputs = tf.cast(image, tf.float32)\n",
    "\t\t\t(convOutputs, predictions) = gradModel(inputs)\n",
    "\t\t\tloss = predictions[:, self.classIdx]\n",
    "\n",
    "\t\t# use automatic differentiation to compute the gradients\n",
    "\t\tgrads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "\t\t# compute the guided gradients\n",
    "\t\tcastConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "\t\tcastGrads = tf.cast(grads > 0, \"float32\")\n",
    "\t\tguidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "\t\t# the convolution and guided gradients have a batch dimension\n",
    "\t\t# (which we don't need) so let's grab the volume itself and\n",
    "\t\t# discard the batch\n",
    "\t\tconvOutputs = convOutputs[0]\n",
    "\t\tguidedGrads = guidedGrads[0]\n",
    "\n",
    "\t\t# compute the average of the gradient values, and using them\n",
    "\t\t# as weights, compute the ponderation of the filters with\n",
    "\t\t# respect to the weights\n",
    "\t\tweights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "\t\tcam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "\t\t# grab the spatial dimensions of the input image and resize\n",
    "\t\t# the output class activation map to match the input image\n",
    "\t\t# dimensions\n",
    "\t\t(w, h) = (image.shape[2], image.shape[1])\n",
    "\t\theatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "\n",
    "\t\t# normalize the heatmap such that all values lie in the range\n",
    "\t\t# [0, 1], scale the resulting values to the range [0, 255],\n",
    "\t\t# and then convert to an unsigned 8-bit integer\n",
    "\t\tnumer = heatmap - np.min(heatmap)\n",
    "\t\tdenom = (heatmap.max() - heatmap.min()) + eps\n",
    "\t\theatmap = numer / denom\n",
    "\t\theatmap = (heatmap * 255).astype(\"uint8\")\n",
    "\n",
    "\t\t# return the resulting heatmap to the calling function\n",
    "\t\treturn heatmap\n",
    "\n",
    "\tdef overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "\t\tcolormap=cv2.COLORMAP_JET):\n",
    "\t\t# apply the supplied color map to the heatmap and then\n",
    "\t\t# overlay the heatmap on the input image\n",
    "\t\theatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "\t\toutput = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "\n",
    "\t\t# return a 2-tuple of the color mapped heatmap and the output,\n",
    "\t\t# overlaid image\n",
    "\t\treturn (heatmap, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_covidtest4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from keras_applications.resnet50 import ResNet50\n",
    "model_builder = keras.applications.resnet50.ResNet50\n",
    "preprocess_input =keras.applications.resnet50.preprocess_input\n",
    "decode_predictions = keras.applications.resnet50.decode_predictions\n",
    "img_size = (256, 256)\n",
    "last_conv_layer_name = \"add_15\"\n",
    "classifier_layer_names = [\n",
    "    \"global_average_pooling2d\",\n",
    "    \"fc3  \",\n",
    "]\n",
    "img_path=cv2.imread(r\"C:\\Users\\Glau\\Desktop\\cam\\test\\3125.jpeg\")\n",
    "#img_path = keras.utils.get_file(\n",
    "    #\"3125.jpeg\",r\"C:\\Users\\Glau\\Desktop\\cam\\test\")\n",
    "#img_path=r\"C:\\Users\\Glau\\Desktop\\cam\\test\\3125.jpeg\"\n",
    "#display(Image(img_path))\n",
    "plt.imshow(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = cv2.imread('img_path')\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = keras.preprocessing.image.img_to_array(img_path)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(\n",
    "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
    "):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = keras.Model(classifier_input, x)\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "    # This is the gradient of the top predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image\n",
    "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
    "\n",
    "\n",
    "# Make model\n",
    "model = model_builder(weights=\"model_covidtest4.h5\")\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img_array)\n",
    "print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(\n",
    "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
    ")\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_covidtest4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=model.load_weights('model_covidtest4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\tdef __init__(self, model, classIdx, layerName=None):\n",
    "\t\t# store the model, the class index used to measure the class\n",
    "\t\t# activation map, and the layer to be used when visualizing\n",
    "\t\t# the class activation map\n",
    "\t\tself.model = model\n",
    "\t\tself.classIdx = classIdx\n",
    "\t\tself.layerName = layerName\n",
    "\n",
    "\t\t# if the layer name is None, attempt to automatically find\n",
    "\t\t# the target output layer\n",
    "\t\tif self.layerName is None:\n",
    "\t\t\tself.layerName = self.find_target_layer()\n",
    "\n",
    "\tdef find_target_layer(self):\n",
    "\t\t# attempt to find the final convolutional layer in the network\n",
    "\t\t# by looping over the layers of the network in reverse order\n",
    "\t\tfor layer in reversed(self.model.layers):\n",
    "\t\t\t# check to see if the layer has a 4D output\n",
    "\t\t\tif len(layer.output_shape) == 4:\n",
    "\t\t\t\treturn layer.name\n",
    "\n",
    "\t\t# otherwise, we could not find a 4D layer so the GradCAM\n",
    "\t\t# algorithm cannot be applied\n",
    "\t\traise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "\n",
    "\tdef compute_heatmap(self, image, eps=1e-8):\n",
    "\t\t# construct our gradient model by supplying (1) the inputs\n",
    "\t\t# to our pre-trained model, (2) the output of the (presumably)\n",
    "\t\t# final 4D layer in the network, and (3) the output of the\n",
    "\t\t# softmax activations from the model\n",
    "\t\tgradModel = Model(\n",
    "\t\t\tinputs=[self.model.inputs],\n",
    "\t\t\toutputs=[self.model.get_layer(self.layerName).output, \n",
    "                     \n",
    "\t\t\t\tself.model.output])\n",
    "\n",
    "\t\t# record operations for automatic differentiation\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\t# cast the image tensor to a float-32 data type, pass the\n",
    "\t\t\t# image through the gradient model, and grab the loss\n",
    "\t\t\t# associated with the specific class index\n",
    "\t\t\tinputs = tf.cast(image, tf.float32)\n",
    "\t\t\t(convOutputs, predictions) = gradModel(inputs)\n",
    "\t\t\tloss = predictions[:, self.classIdx]\n",
    "\n",
    "\t\t# use automatic differentiation to compute the gradients\n",
    "\t\tgrads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "\t\t# compute the guided gradients\n",
    "\t\tcastConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "\t\tcastGrads = tf.cast(grads > 0, \"float32\")\n",
    "\t\tguidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "\t\t# the convolution and guided gradients have a batch dimension\n",
    "\t\t# (which we don't need) so let's grab the volume itself and\n",
    "\t\t# discard the batch\n",
    "\t\tconvOutputs = convOutputs[0]\n",
    "\t\tguidedGrads = guidedGrads[0]\n",
    "\n",
    "\t\t# compute the average of the gradient values, and using them\n",
    "\t\t# as weights, compute the ponderation of the filters with\n",
    "\t\t# respect to the weights\n",
    "\t\tweights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "\t\tcam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "\t\t# grab the spatial dimensions of the input image and resize\n",
    "\t\t# the output class activation map to match the input image\n",
    "\t\t# dimensions\n",
    "\t\t(w, h) = (image.shape[2], image.shape[1])\n",
    "\t\theatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "\n",
    "\t\t# normalize the heatmap such that all values lie in the range\n",
    "\t\t# [0, 1], scale the resulting values to the range [0, 255],\n",
    "\t\t# and then convert to an unsigned 8-bit integer\n",
    "\t\tnumer = heatmap - np.min(heatmap)\n",
    "\t\tdenom = (heatmap.max() - heatmap.min()) + eps\n",
    "\t\theatmap = numer / denom\n",
    "\t\theatmap = (heatmap * 255).astype(\"uint8\")\n",
    "\n",
    "\t\t# return the resulting heatmap to the calling function\n",
    "\t\treturn heatmap\n",
    "\n",
    "\tdef overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "\t\tcolormap=cv2.COLORMAP_JET):\n",
    "\t\t# apply the supplied color map to the heatmap and then\n",
    "\t\t# overlay the heatmap on the input image\n",
    "\t\theatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "\t\toutput = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "\n",
    "\t\t# return a 2-tuple of the color mapped heatmap and the output,\n",
    "\t\t# overlaid image\n",
    "\t\treturn (heatmap, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_image_size=(256,256)\n",
    "img_path=cv2.imread(r\"C:\\Users\\Glau\\Desktop\\cam\\test\\3125.jpeg\")\n",
    "image=cv2.resize(img_path,default_image_size)\n",
    "image=np.array(image/255.0)\n",
    "plt.imshow(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = testX[7]\n",
    "preds = model.predict(image[np.newaxis,...])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(image[np.newaxis,...])\n",
    "i = np.argmax(preds[0])\n",
    "\n",
    "# decode the ImageNet predictions to obtain the human-readable label\n",
    "\n",
    "# # initialize our gradient class activation map and build the heatmap\n",
    "cam = GradCAM(model, i)\n",
    "heatmap = cam.compute_heatmap(image[np.newaxis,...])\n",
    "\n",
    "img_copy = np.copy(image)\n",
    "img_copy -= img_copy.min((0,1))\n",
    "img_copy = (255*img_copy).astype(np.uint8)\n",
    "# resize the resulting heatmap to the original input image dimensions\n",
    "# and then overlay heatmap on top of the image\n",
    "heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "(heatmap, output) = cam.overlay_heatmap(heatmap, img_copy, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output, cmap = 'gray')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_applications.resnext import ResNeXt50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_applications.resnext import ResNeXt50\n",
    "model_builder = ResNeXt50\n",
    "preprocess_input =keras.applications.resnet50.preprocess_input\n",
    "decode_predictions = keras.applications.resnet50.decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
