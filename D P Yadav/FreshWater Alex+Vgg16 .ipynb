{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Add,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D\n",
    "from keras.models import Model,load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('acc') >= 0.96):   \n",
    "            print(\"Reached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS =20\n",
    "INIT_LR = 1e-3\n",
    "BS = 100\n",
    "default_image_size = tuple((100, 100))\n",
    "#image_size = 0\n",
    "width=100\n",
    "height=100\n",
    "depth=3\n",
    "train_dir=r\"C:\\Users\\Glau\\Desktop\\Ashish_Fresh Water\\Train\"\n",
    "valid_dir=r\"C:\\Users\\Glau\\Desktop\\Ashish_Fresh Water\\Test\"\n",
    "train_folder=listdir(train_dir)\n",
    "valid_folder=listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = MyCustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Aphanizomenon ...\n",
      "processing Coelosphaerium ...\n",
      "processing Cylindrospermopsis ...\n",
      "processing Dolichospermum ...\n",
      "processing Euglena ...\n",
      "processing Gloeotrichia ...\n",
      "processing Microcystin ...\n",
      "processing Peridinium ...\n",
      "processing Planktothrix ...\n",
      "processing Scytonema ...\n",
      "[INFO] Image loading completed\n"
     ]
    }
   ],
   "source": [
    "train_image_list, train_image_label= [], []\n",
    "for disease_folder in train_folder:\n",
    "    print(f\"processing {disease_folder} ...\")\n",
    "    disease_img_folder= listdir(f\"{train_dir}/{disease_folder}\")\n",
    "    #print(disease_img_folder)\n",
    "    for disease_img in disease_img_folder:\n",
    "    #for disease_img in disease_img_folder[: : 2]:    \n",
    "        image_directory = f\"{train_dir}/{disease_folder}/{disease_img}\"\n",
    "        if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".TIFF\") == True:\n",
    "            train_image_list.append(convert_image_to_array(image_directory))\n",
    "            train_image_label.append(disease_folder)\n",
    "print(\"[INFO] Image loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Aphanizomenon ...\n",
      "processing Coelosphaerium ...\n",
      "processing Cylindrospermopsis ...\n",
      "processing Dolichospermum ...\n",
      "processing Euglena ...\n",
      "processing Gloeotrichia ...\n",
      "processing Microcystin ...\n",
      "processing Peridinium ...\n",
      "processing Planktothrix ...\n",
      "processing Scytonema ...\n",
      "[INFO] Image loading completed\n"
     ]
    }
   ],
   "source": [
    "valid_image_list, valid_image_label= [], []\n",
    "for disease_folder in valid_folder:\n",
    "    print(f\"processing {disease_folder} ...\")\n",
    "    disease_img_folder= listdir(f\"{valid_dir}/{disease_folder}\")\n",
    "    for disease_img in disease_img_folder:\n",
    "    #for disease_img in disease_img_folder[: : 2]: \n",
    "        image_directory = f\"{valid_dir}/{disease_folder}/{disease_img}\"\n",
    "        if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".TIFF\") == True:\n",
    "            valid_image_list.append(convert_image_to_array(image_directory))\n",
    "            valid_image_label.append(disease_folder)\n",
    "print(\"[INFO] Image loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "bin_train_image_labels = label_binarizer.fit_transform(train_image_label)\n",
    "bin_valid_image_labels = label_binarizer.fit_transform(valid_image_label)\n",
    "pickle.dump(label_binarizer,open('Label_Instance_freshwater_alex+vgg16.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_image_list = np.array(train_image_list, dtype=np.float32) / 255.0\n",
    "np_valid_image_list = np.array(valid_image_list, dtype=np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,classification_report \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(input_shape=(100,100,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model1.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model1.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model1.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model1.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model1.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(100,100,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    #keras.layers.Dense(4096, activation='relu'),\n",
    "    #keras.layers.Dropout(0.5),\n",
    "    #keras.layers.Dense(4096, activation='relu'),\n",
    "    #keras.layers.Dropout(0.5),\n",
    "    #keras.layers.Dense(3, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = Concatenate()([model1.output,model2.output])\n",
    "\n",
    "x = Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(merged_model)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(256, activation = 'relu', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(128, activation = 'relu', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "output = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model = Model(inputs=[model1.input,model2.input],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fusion_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"[INFO] training network...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=fusion_model.fit([np_train_image_list,np_train_image_list,],bin_train_image_labels,\n",
    "                  validation_data=([np_valid_image_list,np_valid_image_list], bin_valid_image_labels),\n",
    "                  batch_size=BS,\n",
    "                  epochs=EPOCHS, verbose=1        \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
