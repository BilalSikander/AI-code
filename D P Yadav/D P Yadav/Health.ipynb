{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('acc') >= 0.96):   \n",
    "            print(\"Reached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS =20\n",
    "INIT_LR = 1e-3\n",
    "BS = 15\n",
    "default_image_size = tuple((224, 224))\n",
    "image_size = 0\n",
    "width=224\n",
    "height=224\n",
    "depth=3\n",
    "train_dir=r\"D:\\DP\\Algae\\Train\"\n",
    "valid_dir=r\"D:\\DP\\Algae\\Test\"\n",
    "train_folder=listdir(train_dir)\n",
    "valid_folder=listdir(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = MyCustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Anabaena ...\n",
      "processing anisonema ...\n",
      "processing Arthrospira ...\n",
      "processing Ceratium ...\n",
      "processing Chlamydomonas ...\n",
      "processing Cyclidiopsis ...\n",
      "processing Desmodesmus ...\n",
      "processing Dunaliella ...\n",
      "processing Eudorina ...\n",
      "processing Glaucophyta ...\n",
      "processing Gymnodinium ...\n",
      "processing Kirchneriella ...\n",
      "processing Oonephris ...\n",
      "processing Oscillatoria ...\n",
      "processing Peridinium ...\n",
      "processing Phacus ...\n",
      "[INFO] Image loading completed\n"
     ]
    }
   ],
   "source": [
    "train_image_list, train_image_label= [], []\n",
    "for disease_folder in train_folder:\n",
    "    print(f\"processing {disease_folder} ...\")\n",
    "    disease_img_folder= listdir(f\"{train_dir}/{disease_folder}\")\n",
    "    #print(disease_img_folder)\n",
    "    for disease_img in disease_img_folder:\n",
    "    #for disease_img in disease_img_folder[: : 2]:    \n",
    "        image_directory = f\"{train_dir}/{disease_folder}/{disease_img}\"\n",
    "        if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".bmp\") == True:\n",
    "            train_image_list.append(convert_image_to_array(image_directory))\n",
    "            train_image_label.append(disease_folder)\n",
    "print(\"[INFO] Image loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65001\n"
     ]
    }
   ],
   "source": [
    "print(len(train_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Anabaena ...\n",
      "processing anisonema ...\n",
      "processing Arthrospira ...\n",
      "processing Ceratium ...\n",
      "processing Chlamydomonas ...\n",
      "processing Cyclidiopsis ...\n",
      "processing Desmodesmus ...\n",
      "processing Dunaliella ...\n",
      "processing Eudorina ...\n",
      "processing Glaucophyta ...\n",
      "processing Gymnodinium ...\n",
      "processing Kirchneriella ...\n",
      "processing Oonephris ...\n",
      "processing Oscillatoria ...\n",
      "processing Peridinium ...\n",
      "processing Phacus ...\n",
      "[INFO] Image loading completed\n"
     ]
    }
   ],
   "source": [
    "valid_image_list, valid_image_label= [], []\n",
    "for disease_folder in valid_folder:\n",
    "    print(f\"processing {disease_folder} ...\")\n",
    "    disease_img_folder= listdir(f\"{valid_dir}/{disease_folder}\")\n",
    "    for disease_img in disease_img_folder:\n",
    "    #for disease_img in disease_img_folder[: : 2]: \n",
    "        image_directory = f\"{valid_dir}/{disease_folder}/{disease_img}\"\n",
    "        if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".bmp\") == True:\n",
    "            valid_image_list.append(convert_image_to_array(image_directory))\n",
    "            valid_image_label.append(disease_folder)\n",
    "print(\"[INFO] Image loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "bin_train_image_labels = label_binarizer.fit_transform(train_image_label)\n",
    "bin_valid_image_labels = label_binarizer.fit_transform(valid_image_label)\n",
    "pickle.dump(label_binarizer,open('Label_Instance_alage_resgan_12.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_image_list = np.array(train_image_list, dtype=np.float32) / 255.0\n",
    "np_valid_image_list = np.array(valid_image_list, dtype=np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1, 1, 2048)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 1, 3136)   6425664     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 1, 1, 3136)   0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 7, 7, 64)     0           leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 64)     4160        reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 7, 7, 64)     0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 14, 14, 64)   36928       leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, 14, 14, 64)   0           conv2d_transpose_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 14, 14, 256)  16640       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 14, 14, 256)  16640       leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 14, 14, 256)  0           conv2d_transpose_11[0][0]        \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 14, 14, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 14, 14, 64)   16448       leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 14, 14, 64)   0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 14, 14, 64)   36928       leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 14, 14, 64)   0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 14, 14, 256)  16640       leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 14, 14, 256)  0           leaky_re_lu_54[0][0]             \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 14, 14, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 14, 14, 128)  32896       leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, 14, 14, 128)  0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 28, 28, 128)  147584      leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 28, 28, 128)  0           conv2d_transpose_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 28, 28, 512)  131584      leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 28, 28, 512)  66048       leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 28, 28, 512)  0           conv2d_transpose_13[0][0]        \n",
      "                                                                 conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, 28, 28, 512)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 28, 28, 128)  65664       leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 28, 28, 128)  0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 28, 28, 128)  147584      leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 28, 28, 128)  0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 28, 28, 512)  66048       leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 28, 28, 512)  0           leaky_re_lu_60[0][0]             \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 28, 28, 512)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 28, 28, 256)  131328      leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 28, 28, 256)  0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 56, 56, 256)  590080      leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 56, 56, 256)  0           conv2d_transpose_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 56, 56, 1024) 525312      leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 56, 56, 1024) 263168      leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 56, 56, 1024) 0           conv2d_transpose_15[0][0]        \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 56, 56, 1024) 0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 56, 56, 256)  262400      leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 56, 56, 256)  0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 56, 56, 256)  590080      leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 56, 56, 256)  0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 56, 56, 1024) 263168      leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 56, 56, 1024) 0           leaky_re_lu_66[0][0]             \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 56, 56, 1024) 0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 56, 56, 512)  524800      leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 56, 56, 512)  0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTran (None, 112, 112, 512 2359808     leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 112, 112, 512 0           conv2d_transpose_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTran (None, 112, 112, 204 2099200     leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 112, 112, 204 1050624     leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 112, 112, 204 0           conv2d_transpose_17[0][0]        \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 112, 112, 204 0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 112, 112, 512 1049088     leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)      (None, 112, 112, 512 0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 112, 112, 512 2359808     leaky_re_lu_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)      (None, 112, 112, 512 0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 112, 112, 204 1050624     leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 112, 112, 204 0           leaky_re_lu_72[0][0]             \n",
      "                                                                 conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)      (None, 112, 112, 204 0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DTran (None, 224, 224, 3)  301059      leaky_re_lu_75[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 20,648,003\n",
      "Trainable params: 20,648,003\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 112, 112, 64) 9472        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)      (None, 112, 112, 64) 0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 112, 112, 64) 4160        leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)      (None, 112, 112, 64) 0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 56, 56, 64)   36928       leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)      (None, 56, 56, 64)   0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 56, 56, 256)  16640       leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 56, 56, 256)  16640       leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 56, 56, 256)  0           conv2d_74[0][0]                  \n",
      "                                                                 conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)      (None, 56, 56, 256)  0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 56, 56, 64)   16448       leaky_re_lu_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)      (None, 56, 56, 64)   0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 56, 56, 64)   36928       leaky_re_lu_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_81 (LeakyReLU)      (None, 56, 56, 64)   0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 56, 56, 256)  16640       leaky_re_lu_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 56, 56, 256)  0           leaky_re_lu_79[0][0]             \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_82 (LeakyReLU)      (None, 56, 56, 256)  0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 56, 56, 128)  32896       leaky_re_lu_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_83 (LeakyReLU)      (None, 56, 56, 128)  0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 28, 28, 128)  147584      leaky_re_lu_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)      (None, 28, 28, 128)  0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 28, 28, 512)  131584      leaky_re_lu_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 28, 28, 512)  66048       leaky_re_lu_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 28, 28, 512)  0           conv2d_81[0][0]                  \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)      (None, 28, 28, 512)  0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 28, 28, 128)  65664       leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)      (None, 28, 28, 128)  0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 28, 28, 128)  147584      leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)      (None, 28, 28, 128)  0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 28, 28, 512)  66048       leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 28, 28, 512)  0           leaky_re_lu_85[0][0]             \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)      (None, 28, 28, 512)  0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 28, 28, 256)  131328      leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)      (None, 28, 28, 256)  0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 14, 14, 256)  590080      leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_90 (LeakyReLU)      (None, 14, 14, 256)  0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 14, 14, 1024) 525312      leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 14, 14, 1024) 263168      leaky_re_lu_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 14, 14, 1024) 0           conv2d_88[0][0]                  \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_91 (LeakyReLU)      (None, 14, 14, 1024) 0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 14, 14, 256)  262400      leaky_re_lu_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_92 (LeakyReLU)      (None, 14, 14, 256)  0           conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 14, 14, 256)  590080      leaky_re_lu_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)      (None, 14, 14, 256)  0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 14, 14, 1024) 263168      leaky_re_lu_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 14, 14, 1024) 0           leaky_re_lu_91[0][0]             \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)      (None, 14, 14, 1024) 0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 14, 14, 512)  524800      leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_95 (LeakyReLU)      (None, 14, 14, 512)  0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 7, 7, 512)    2359808     leaky_re_lu_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)      (None, 7, 7, 512)    0           conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 7, 7, 2048)   2099200     leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 7, 7, 2048)   1050624     leaky_re_lu_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 7, 7, 2048)   0           conv2d_95[0][0]                  \n",
      "                                                                 conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)      (None, 7, 7, 2048)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 7, 7, 512)    1049088     leaky_re_lu_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_98 (LeakyReLU)      (None, 7, 7, 512)    0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 7, 7, 512)    2359808     leaky_re_lu_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_99 (LeakyReLU)      (None, 7, 7, 512)    0           conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 7, 7, 2048)   1050624     leaky_re_lu_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 7, 7, 2048)   0           leaky_re_lu_97[0][0]             \n",
      "                                                                 conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_100 (LeakyReLU)     (None, 7, 7, 2048)   0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 2048)         0           leaky_re_lu_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           32784       global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 13,963,536\n",
      "Trainable params: 13,963,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"combined\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1, 1, 2048)        0         \n",
      "_________________________________________________________________\n",
      "generator (Model)            (None, 224, 224, 3)       20648003  \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 16)                13963536  \n",
      "=================================================================\n",
      "Total params: 34,611,539\n",
      "Trainable params: 34,611,539\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#\n",
    "# generator input params\n",
    "#\n",
    "\n",
    "rand_dim = (1, 1, 2048)  # dimension of the generator's input tensor (gaussian noise)\n",
    "\n",
    "#\n",
    "# image dimensions\n",
    "#\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "img_channels = 3\n",
    "\n",
    "#\n",
    "# shared network params\n",
    "#\n",
    "\n",
    "cardinality = 1\n",
    "\n",
    "\n",
    "def add_common_layers(y):\n",
    "    # batch norm is commented out b/c of keras bugs\n",
    "    # y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def grouped_convolution(y, nb_channels, _strides, _transposed=False):\n",
    "    # when `cardinality` == 1 this is just a standard convolution\n",
    "    if cardinality == 1:\n",
    "        if _strides != (1, 1) and _transposed:\n",
    "            return layers.Conv2DTranspose(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        else:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "\n",
    "    assert not nb_channels % cardinality\n",
    "    _d = nb_channels // cardinality\n",
    "\n",
    "    # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "    # and convolutions are separately performed within each group\n",
    "    groups = []\n",
    "    for j in range(cardinality):\n",
    "        group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "        if _strides != (1, 1) and _transposed:\n",
    "            groups.append(layers.Conv2DTranspose(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "        else:\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "\n",
    "    # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "    y = layers.concatenate(groups)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False, _transposed=False):\n",
    "    \"\"\"\n",
    "    Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "    and are subject to two simple rules:\n",
    "    - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "    - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "      * If up-sampled in case of `_transposed` == True, the width of the blocks is divided by a factor of 2.\n",
    "    \n",
    "    \"\"\"\n",
    "    shortcut = y\n",
    "\n",
    "    # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "    y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "    y = grouped_convolution(y, nb_channels_in, _strides=_strides, _transposed=_transposed)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "    # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "    # y = layers.BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        if _strides != (1, 1) and _transposed:\n",
    "            shortcut = layers.Conv2DTranspose(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        else:\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "\n",
    "        # shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    y = layers.add([shortcut, y])\n",
    "\n",
    "    # relu is performed right after each batch normalization,\n",
    "    # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "    y = layers.LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def stack_blocks(x, transposed=False):\n",
    "    # conv2\n",
    "    for i in range(2):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 64, 256, _strides=strides, _transposed=transposed)\n",
    "\n",
    "    # conv3\n",
    "    for i in range(2):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 128, 512, _strides=strides, _transposed=transposed)\n",
    "\n",
    "    # conv4\n",
    "    for i in range(2):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 256, 1024, _strides=strides, _transposed=transposed)\n",
    "\n",
    "    # conv5\n",
    "    for i in range(2):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 512, 2048, _strides=strides, _transposed=transposed)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def generator_network(x):\n",
    "    x = layers.Dense(64 * 7 * 7)(x)\n",
    "    x = add_common_layers(x)\n",
    "\n",
    "    x = layers.Reshape((7, 7, 64))(x)\n",
    "    x = stack_blocks(x, transposed=True)\n",
    "\n",
    "    # (conv1 disc)\n",
    "    # number of feature maps => number of image channels\n",
    "    return layers.Conv2DTranspose(img_channels, kernel_size=(7, 7), strides=(2, 2), padding='same', activation='tanh')(x)\n",
    "\n",
    "\n",
    "def discriminator_network(x):\n",
    "    \"\"\"\n",
    "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    # conv1\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "\n",
    "    x = stack_blocks(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(16)(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "\n",
    "#\n",
    "# define model input and output tensors\n",
    "#\n",
    "\n",
    "generator_input_tensor = layers.Input(shape=rand_dim)\n",
    "generated_image_tensor = generator_network(generator_input_tensor)\n",
    "\n",
    "generated_or_real_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "discriminator_output = discriminator_network(generated_or_real_image_tensor)\n",
    "\n",
    "#\n",
    "# define models\n",
    "#\n",
    "\n",
    "generator_model = models.Model(inputs=[generator_input_tensor], outputs=[generated_image_tensor], name='generator')\n",
    "discriminator_model = models.Model(inputs=[generated_or_real_image_tensor],\n",
    "                                   outputs=[discriminator_output],\n",
    "                                   name='discriminator')\n",
    "\n",
    "combined_output = discriminator_model(generator_model(generator_input_tensor))\n",
    "combined_model = models.Model(inputs=[generator_input_tensor], outputs=[combined_output], name='combined')\n",
    "\n",
    "print(generator_model.summary())\n",
    "print(discriminator_model.summary())\n",
    "print(combined_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "Clean and simple Keras implementation of network architectures described in:\n",
    "\n",
    "\n",
    "    - (ResNet-50) [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf).\n",
    "\n",
    "\n",
    "    - (ResNeXt-50 32x4d) [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf).\n",
    "\n",
    "\n",
    "Python 3.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "#\n",
    "# image dimensions\n",
    "#\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_channels = 3\n",
    "#\n",
    "# network params\n",
    "#\n",
    "cardinality = 32\n",
    "d = 4  # bottleneck width\n",
    "def residual_network(x, architecture='ResNet'):\n",
    "\n",
    "    if architecture.lower() == 'resnet':\n",
    "\n",
    "        channel_scale_factor = 2\n",
    "\n",
    "        grouped = False\n",
    "\n",
    "    elif architecture.lower() == 'resnext':\n",
    "\n",
    "        channel_scale_factor = 1\n",
    "\n",
    "        grouped = True\n",
    "\n",
    "    else:\n",
    "        assert False, 'Invalid architecture specified. Must be one of { \\'ResNet\\', \\'ResNeXt\\' }.'\n",
    "    def add_common_layers(y):\n",
    "\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        y = layers.LeakyReLU()(y)\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "\n",
    "        assert not nb_channels % cardinality\n",
    "\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "        return y\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False, _grouped=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "      # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "\n",
    "        y = add_common_layers(y)\n",
    "        if not _grouped:\n",
    "            # ResNet\n",
    "            y = layers.Conv2D(nb_channels_in, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        else:\n",
    "            # ResNeXt\n",
    "            y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "       # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions\n",
    "\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization\n",
    "\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "    # conv1\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "  # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 128 // channel_scale_factor, 256, _project_shortcut=project_shortcut, _grouped=grouped)\n",
    "    # conv3\n",
    "    for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 256 // channel_scale_factor,512, _strides=strides, _grouped=grouped)\n",
    "    # conv4\n",
    "    for i in range(6):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 512 // channel_scale_factor, 1024, _strides=strides, _grouped=grouped)\n",
    "    # conv5\n",
    "    for i in range(3):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 1024 // channel_scale_factor, 2048, _strides=strides, _grouped=grouped)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(16)(x)\n",
    "    return x\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "network_output = residual_network(image_tensor, architecture='ResNeXt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "#\n",
    "# image dimensions\n",
    "#\n",
    "\n",
    "img_height = 100\n",
    "img_width = 100\n",
    "img_channels = 3\n",
    "\n",
    "#\n",
    "# network params\n",
    "#\n",
    "\n",
    "cardinality = 32\n",
    "\n",
    "\n",
    "def residual_network(x):\n",
    "    \"\"\"\n",
    "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization,\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # conv1\n",
    "    x = layers.Conv2D(256, kernel_size=(5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "\n",
    "    # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    for i in range(3):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 64, 128, _project_shortcut=project_shortcut)\n",
    "\n",
    "    # conv3\n",
    "    for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 128, 256, _strides=strides)\n",
    "\n",
    "    # conv4\n",
    "    for i in range(6):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 256, 512, _strides=strides)\n",
    "\n",
    "    # conv5\n",
    "    for i in range(3):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 512, 2048, _strides=strides)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(2,activation='softmax')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "network_output = residual_network(image_tensor)\n",
    "  \n",
    "model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "combined_model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"[INFO] training network...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_3 to have shape (1, 1, 2048) but got array with shape (224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c5577c01b868>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_valid_image_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_valid_image_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                   )\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_3 to have shape (1, 1, 2048) but got array with shape (224, 224, 3)"
     ]
    }
   ],
   "source": [
    "history=combined_model.fit(np_train_image_list,bin_train_image_labels,\n",
    "                  validation_data=(np_valid_image_list, bin_valid_image_labels),\n",
    "                  batch_size=BS,\n",
    "                  epochs=EPOCHS, verbose=1        \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model.evaluate(np_valid_image_list, bin_valid_image_labels)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model.evaluate(np_valid_image_list, bin_valid_image_labels)\n",
    "print(f\" std Test Accuracy: numpy.std{scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#Train and validation accuracy\n",
    "plt.plot(epochs, acc, 'g', label='Training accurarcy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#Train and validation loss\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycalculated = model.predict(np_valid_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_valid_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_classes =np.argmax(ycalculated,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rounded_labels=np.argmax(bin_valid_image_labels,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "#import seaborn as sns\n",
    "#y_pred=model.predict_classes(np_valid_image_list)\n",
    "#con_mat = tf.math.confusion_matrix(labels=y_true, predictions=ycalculated).numpy()\n",
    "#matrix = confusion_matrix(rounded_labels,yhat_classes)\n",
    "#y_pred=model.predict(np.array(val_image_list))\n",
    "cm =confusion_matrix(bin_valid_image_labels,yhat_classes)  \n",
    "\n",
    "#matrix =confusion_matrix(bin_val_image_labels.argmax(axis=1), ycalculated)\n",
    "#print(matrix)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re=recall_score(bin_valid_image_labels,yhat_classes,average='binary') \n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(bin_valid_image_labels,yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm,annot=True,cbar=False,fmt=\"d\")\n",
    "plt.ylabel('true positive')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "rc_score=roc_auc_score(bin_valid_image_labels,yhat_classes) \n",
    "print(rc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report \n",
    "target_names = ['graft', 'non-graft']\n",
    "print(classification_report(bin_valid_image_labels, yhat_classes, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    " \n",
    "fpr , tpr , thresholds = roc_curve(bin_valid_image_labels,yhat_classes)\n",
    "def plot_roc_curve(fpr,tpr): \n",
    "  plt.plot(fpr,tpr) \n",
    "  plt.axis([0,1,0,1]) \n",
    "  plt.xlabel('False Positive Rate') \n",
    "  plt.ylabel('True Positive Rate') \n",
    "  plt.show()    \n",
    "  \n",
    "plot_roc_curve (fpr,tpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(bin_valid_image_labels, yhat_classes)\n",
    "auc_rf = auc(fpr_rf, tpr_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(bin_valid_image_labels, yhat_classes)\n",
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_burn_resorignl_123.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "#import seaborn as sns\n",
    "#y_pred=model.predict_classes(np_valid_image_list)\n",
    "#con_mat = tf.math.confusion_matrix(labels=y_true, predictions=ycalculated).numpy()\n",
    "#matrix = confusion_matrix(rounded_labels,yhat_classes)\n",
    "#y_pred=model.predict(np.array(val_image_list))\n",
    "#yhat_classes = [\"honda\", \"chevrolet\", \"honda\", \"toyota\", \"toyota\", \"chevrolet\"]\n",
    "#rounded_labels = [\"honda\", \"chevrolet\", \"honda\", \"toyota\", \"toyota\", \"honda\"]\n",
    "cm =confusion_matrix(rounded_labels,yhat_classes)  \n",
    "\n",
    "#matrix =confusion_matrix(bin_val_image_labels.argmax(axis=1), ycalculated)\n",
    "#print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report \n",
    "target_names = ['Graft', 'Non-Graft']\n",
    "print(classification_report(rounded_labels, yhat_classes, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "\n",
    "bg_color = (0.88,0.85,0.95)\n",
    "plt.rcParams['figure.facecolor'] = bg_color\n",
    "plt.rcParams['axes.facecolor'] = bg_color\n",
    "sns.heatmap(cm,annot=True,cbar=False,fmt=\"d\",annot_kws={'size':10},cmap='coolwarm')\n",
    "#res.invert_yaxis()\n",
    "\n",
    "#plt.yticks([0.5,1.5,2.5], [ 'Dog', 'Cat', 'Rabbit'],va='center')\n",
    "\n",
    "plt.ylabel('true positive')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Heat map Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(rounded_labels,yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test= lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "multiclass_roc_auc_score(rounded_labels,yhat_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "n_classes =3\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 2\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(rounded_labels)\n",
    "y_test= lb.transform(rounded_labels)\n",
    "snn_pred = lb.transform(yhat_classes)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], snn_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), snn_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes-97), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(3), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw,color='white')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic of Cell')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_health_123.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "#img = x_test[13]\n",
    "img = load_img(r\"C:\\Users\\Glau\\Desktop\\DP\\cancer_1.jpg\", target_size=(256, 256))\n",
    "plt.imshow(img)\n",
    "plt.grid(True)\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = load_model(\"model_ped123.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# redefine model to output right after the first hidden layer\n",
    "ixs = [2, 5, 9, 13, 17]\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "model = Model(inputs=model.inputs, outputs=outputs)\n",
    "# load the image with the required shape\n",
    "img = load_img(r\"C:\\Users\\Glau\\Desktop\\DP\\cancer_1.jpg\", target_size=(224, 224))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot the output from each block\n",
    "square = 8\n",
    "for fmap in feature_maps:\n",
    "\t# plot all 64 maps in an 8x8 squares\n",
    "\tix = 1\n",
    "\tfor _ in range(square):\n",
    "\t\tfor _ in range(square):\n",
    "\t\t\t# specify subplot and turn of axis\n",
    "\t\t\tax = pyplot.subplot(square, square, ix)\n",
    "\t\t\tax.set_xticks([])\n",
    "\t\t\tax.set_yticks([])\n",
    "\t\t\t# plot filter channel in grayscale\n",
    "\t\t\tpyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
    "\t\t\tix += 1\n",
    "\t# show the figure\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_cancer_New12.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
